{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Removal of Similar Frame**"
      ],
      "metadata": {
        "id": "LLQ-AQPvAfQD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "zjyGJAsaaAWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install vulture"
      ],
      "metadata": {
        "id": "jUZ_NrIud2Jy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VRzY7hdY_svB"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import pathlib\n",
        "import os\n",
        "video_path='path to input video'\n",
        "# capture = cv2.VideoCapture(video_path)\n",
        "video_size_mb = os.path.getsize(pathlib.Path(video_path)) / (1024 * 1024)\n",
        "\n",
        "print(f\"Video size: {video_size_mb:.2f} MB\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "bpSOph55BsII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "\n",
        "# Input and output paths\n",
        "input_video_path = '/content/drive/MyDrive/Malayalam-Sita-Ramam-Full-screen.mp4'\n",
        "output_video_path = '/content/output.mp4'\n",
        "\n",
        "# Get initial video size\n",
        "initial_size = os.path.getsize(input_video_path)\n",
        "\n",
        "# Load video\n",
        "cap = cv2.VideoCapture(input_video_path)\n",
        "\n",
        "# Get video properties\n",
        "width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')"
      ],
      "metadata": {
        "id": "g79-2y_LUd8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pipeline for FPS Reduction**"
      ],
      "metadata": {
        "id": "x7jJ_Z_tUfk8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "if(fps<=15):\n",
        "  fps=15\n",
        "elif(fps>15 and fps<=30):\n",
        "  fps=fps\n",
        "else:\n",
        "  fps=fps//2"
      ],
      "metadata": {
        "id": "TLyKRjUyUqyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
        "\n",
        "frames = []\n",
        "frames1=[]\n",
        "initial_frame_count = 0\n",
        "j=0\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    initial_frame_count += 1\n",
        "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "    gray_frame_flat = gray_frame.flatten()\n",
        "\n",
        "    if frames:\n",
        "        prev_frame = frames[-1]\n",
        "        similarity = cosine_similarity([gray_frame_flat], [prev_frame])\n",
        "        if similarity >1.1:\n",
        "            continue\n",
        "\n",
        "    frames.append(gray_frame_flat)\n",
        "\n",
        "    # Resize frame to lower resolution\n",
        "    frame = cv2.resize(frame, (width, height))\n",
        "    # frame = cv2.fastNlMeansDenoisingColored(frame, None, 10, 10, 7, 21)\n",
        "    frames1.append(frame)\n",
        "    j+=1\n",
        "    # cv2.imwrite('/content/dhoni/new/'+str(j)+'.jpeg',frame)\n",
        "    out.write(frame)\n",
        "\n",
        "cap.release()\n",
        "out.release()\n",
        "\n",
        "# Get final video size\n",
        "final_size = os.path.getsize(output_video_path)\n",
        "\n",
        "print(f\"Initial video size: {initial_size / (1024 * 1024):.2f} MB\")\n",
        "print(f\"Final video size: {final_size / (1024 * 1024):.2f} MB\")\n",
        "print(f\"Initial frame count: {initial_frame_count}\")\n",
        "print(f\"Final frame count: {len(frames)}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "EAg1Fbn_Bugx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Extraction of faces from video using caffe model**"
      ],
      "metadata": {
        "id": "urPx0BniCwa7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "iNnKiMYdCOls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd \"/content\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IzSVvzdTChVx",
        "outputId": "f23ee3c4-267b-4099-d9f6-d921a70484ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone \"https://github.com/kb22/Create-Face-Data-from-Images\""
      ],
      "metadata": {
        "id": "Ki9vGFlQCkHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd \"/content/Create-Face-Data-from-Images\""
      ],
      "metadata": {
        "id": "Z4z7QXu8Cm0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load Caffe Model Path**"
      ],
      "metadata": {
        "id": "yiQDk-PWFJQI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prototxt_path = '/content/Create-Face-Data-from-Images/model_data/deploy.prototxt'\n",
        "caffemodel_path = '/content/Create-Face-Data-from-Images/model_data/weights.caffemodel'\n",
        "\n",
        "# Read the model\n",
        "model = cv2.dnn.readNetFromCaffe(prototxt_path, caffemodel_path)"
      ],
      "metadata": {
        "id": "eIqJ7QNTCpPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create Directory to extract faces**"
      ],
      "metadata": {
        "id": "3MEdL6ItFUbu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists('faces'):\n",
        "\tprint(\"New directory created\")\n",
        "\tos.makedirs('faces')"
      ],
      "metadata": {
        "id": "caefRNqZCvhk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Deepface Model Installation**"
      ],
      "metadata": {
        "id": "sBOsyGj3Fo4t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install deepface"
      ],
      "metadata": {
        "id": "8d0P-GZhpPIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd \"/content\"\n",
        "! git clone \"https://github.com/serengil/deepface.git\"\n",
        "%cd deepface\n",
        "!pip install -e ."
      ],
      "metadata": {
        "id": "X95RFeHTpRrm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from deepface import DeepFace"
      ],
      "metadata": {
        "id": "t1bvM72upVLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loading of Haarcascade model**"
      ],
      "metadata": {
        "id": "FP14r_5BF2dC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "body_classifier = cv2.CascadeClassifier('/content/haarcascade_fullbody.xml')"
      ],
      "metadata": {
        "id": "cDj-iYX55kC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Get video height and width**"
      ],
      "metadata": {
        "id": "-PN1QvBLGUQb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "frames1=np.asarray(frames1)\n",
        "(h, w) = frames1.shape[1:3]\n",
        "print(\"Height of frame=\",h)\n",
        "print(\"Width of frame=\",w)"
      ],
      "metadata": {
        "id": "qO7vz9l7GTNc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pipeline for frame elimination**"
      ],
      "metadata": {
        "id": "hc2XgMLZGZrm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1 Character Verified**"
      ],
      "metadata": {
        "id": "ZbwtgJkrbSxz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "l=0\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "output_video_path=\"/content/001output.mp4\"\n",
        "out = cv2.VideoWriter(output_video_path, fourcc, fps, (w,h))\n",
        "# all frames of video stored in frames 1\n",
        "for image in frames1:\n",
        "  #image is one frame among frame1\n",
        "  #extraction of face from image\n",
        "  blob = cv2.dnn.blobFromImage(cv2.resize(image, (300, 300)), 1.0, (300, 300), (104.0, 177.0, 123.0))\n",
        "  model.setInput(blob)\n",
        "  detections = model.forward()\n",
        "  flag=0\n",
        "  flag1=0\n",
        "  fme=image\n",
        "  gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "  #detect body in frame and return their coordinates\n",
        "  bodies = body_classifier.detectMultiScale(gray, 1.2, 3)\n",
        "  if(len(bodies)!=0):\n",
        "    flag1=1\n",
        "  for i in range(0, detections.shape[2]):\n",
        "    box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
        "    (startX, startY, endX, endY) = box.astype(\"int\")\n",
        "    confidence = detections[0, 0, i, 2]\n",
        "    # If confidence > 0.5, show box around face\n",
        "    if (confidence > 0.5):\n",
        "      l+=1\n",
        "      img = image[startY:endY,startX:endX]\n",
        "      print(img.shape)\n",
        "      #img is the frame with face with some additioal attributes\n",
        "      if img is img.shape[0] == 0 or img.shape[1] == 0:\n",
        "        continue\n",
        "      cv2.imwrite('/content/faces1/'+str(l)+'.jpeg', img)\n",
        "      #for verification of extracted face with database\n",
        "      result=DeepFace.find(img_path = '/content/faces1/'+str(l)+'.jpeg', db_path = \"/content/drive/MyDrive/1_character\",enforce_detection = False)\n",
        "      # return the dataframe if it found\n",
        "      print(result.columns)\n",
        "      if(result[0].shape[0]==0):\n",
        "         flag=1\n",
        "  #this condition if there is human in frame and his face is also detected but not verified\n",
        "  if(flag==0 and flag1==1):\n",
        "    out.write(image)\n",
        "  #this conditon if there is no face get detected in the image but there is presence of human in frame\n",
        "  if(flag==1):\n",
        "    out.write(image)\n",
        "    print('!')\n",
        "  #in other condition we eliminate the frame\n",
        "out.release()"
      ],
      "metadata": {
        "id": "bNb5sL-3DkFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Final Video Size**"
      ],
      "metadata": {
        "id": "HWekbkAOBUCR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_size = os.path.getsize('/content/001output.mp4')\n",
        "print(f\"Final video size: {final_size / (1024 * 1024):.2f} MB\")"
      ],
      "metadata": {
        "id": "NEWahZ2lZNVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2 character elimination**"
      ],
      "metadata": {
        "id": "VCXkHRJkbmWK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "l=0\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "output_video_path=\"/content/002output.mp4\"\n",
        "out = cv2.VideoWriter(output_video_path, fourcc, fps, (w,h))\n",
        "# all frames of video stored in frames 1\n",
        "for image in frames1:\n",
        "  #image is one frame among frame1\n",
        "  #extraction of face from image\n",
        "  blob = cv2.dnn.blobFromImage(cv2.resize(image, (300, 300)), 1.0, (300, 300), (104.0, 177.0, 123.0))\n",
        "  model.setInput(blob)\n",
        "  detections = model.forward()\n",
        "  flag=0\n",
        "  flag1=0\n",
        "  fme=image\n",
        "  gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "  #detect body in frame and return their coordinates\n",
        "  bodies = body_classifier.detectMultiScale(gray, 1.2, 3)\n",
        "  if(len(bodies)!=0):\n",
        "    flag1=1\n",
        "  for i in range(0, detections.shape[2]):\n",
        "    box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
        "    (startX, startY, endX, endY) = box.astype(\"int\")\n",
        "    confidence = detections[0, 0, i, 2]\n",
        "    # If confidence > 0.5, show box around face\n",
        "    if (confidence > 0.5):\n",
        "      l+=1\n",
        "      img = image[startY:endY,startX:endX]\n",
        "      print(img.shape)\n",
        "      #img is the frame with face with some additioal attributes\n",
        "      if img is img.shape[0] == 0 or img.shape[1] == 0:\n",
        "        continue\n",
        "      cv2.imwrite('/content/faces2/'+str(l)+'.jpeg', img)\n",
        "      #for verification of extracted face with database\n",
        "      result=DeepFace.find(img_path = '/content/faces2/'+str(l)+'.jpeg', db_path = \"/content/drive/MyDrive/2_character\",enforce_detection = False)\n",
        "      # return the dataframe if it found\n",
        "      if(result[0].shape[0]==0):\n",
        "         flag=1\n",
        "  #this condition if there is human in frame and his face is also detected but not verified\n",
        "  if(flag==0 and flag1==1):\n",
        "    out.write(image)\n",
        "  #this conditon if there is no face get detected in the image but there is presence of human in frame\n",
        "  if(flag==1):\n",
        "    out.write(image)\n",
        "    print('!')\n",
        "  #in other condition we eliminate the frame\n",
        "out.release()"
      ],
      "metadata": {
        "id": "UYXuDtLbbb2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Final Video Size**"
      ],
      "metadata": {
        "id": "BHIznTXibg3t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_size = os.path.getsize('/content/002output.mp4')\n",
        "print(f\"Final video size: {final_size / (1024 * 1024):.2f} MB\")"
      ],
      "metadata": {
        "id": "m7cBZW94belK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}